{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from azureml.automl.core.constants import FeaturizationConfigMode\n",
    "from azureml.automl.core.featurization import FeaturizationConfig\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import azureml.core\n",
    "from azureml.core.experiment import Experiment\n",
    "from azureml.core.workspace import Workspace\n",
    "from azureml.train.automl import AutoMLConfig\n",
    "from azureml.core.dataset import Dataset\n",
    "\n",
    "import azureml.dataprep\n",
    "print(azureml.dataprep.__version__)\n",
    "import azureml.core\n",
    "print(azureml.core.VERSION)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace\n",
    "\n",
    "ws = Workspace.from_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Datastore\n",
    "\n",
    "sql_datastore = Datastore.get(workspace=ws, datastore_name=\"ado_sql_datastore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Dataset\n",
    "from azureml.data.datapath import DataPath\n",
    "query = DataPath(sql_datastore, 'SELECT *  FROM Improvements')\n",
    "improvements_sql_ds = Dataset.Tabular.from_sql_query(query)\n",
    "\n",
    "improvements_sql_ds.register(workspace=ws,\n",
    "                             name=\"ai_ag_ado_improvements\",\n",
    "                             description = \"Improvements from Azure DevOps\",\n",
    "                             create_new_version=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Dataset\n",
    "\n",
    "label =\"dc_impact_score\"\n",
    "query_string = 'SELECT *, (POWER(1.5,MitigationScore) * POWER(2,Priority) * POWER(6.585, IsBlocker)) as dc_impact_score FROM FeedbackItems'\n",
    "\n",
    "query = DataPath(sql_datastore, query_string)\n",
    "feedback_sql_ds = Dataset.Tabular.from_sql_query(query)\n",
    "\n",
    "feedback_sql_ds.register(workspace=ws,\n",
    "                         name=\"ai_ag_ado_feedack\",\n",
    "                         description = \"Feedback from Azure DevOps\",\n",
    "                         create_new_version=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feedback_sql_pd = feedback_sql_ds.to_pandas_dataframe()\n",
    "# \n",
    "# label =\"dc_impact_score\"\n",
    "# \n",
    "# def dc_impact_score_calculation(mitigation_score, priority, is_blocker):\n",
    "#     return (1.5**mitigation_score) * (2**priority) * (6.585**is_blocker)\n",
    "#     \n",
    "# \n",
    "# feedback_sql_pd[label] = dc_impact_score_calculation(feedback_sql_pd['MitigationScore'], feedback_sql_pd['Priority'], feedback_sql_pd['IsBlocker'])\n",
    "# \n",
    "# file=\"temp\"\n",
    "# feedback_sql_pd.to_csv(file)\n",
    "# feedback_sql_ds_labeled = Dataset.Tabular.from_delimited_files(path=file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(dataset):\n",
    "    # Split the dataset into train and test datasets\n",
    "    train_data, test_data = dataset.random_split(percentage=0.8, seed=223)\n",
    "\n",
    "    # Register the train dataset with your workspace\n",
    "    train_data.register(workspace = ws, \n",
    "                        name = 'ai_ag_ado_feedack_train_dataset',\n",
    "                        description = 'Feedback from Azure DevOps training data',\n",
    "                        create_new_version=True)\n",
    "\n",
    "    # Register the test dataset with your workspace\n",
    "    test_data.register(workspace = ws, \n",
    "                       name = 'ai_ag_ado_feedack_test_dataset', \n",
    "                       description = 'Feedback from Azure DevOps test data',\n",
    "                       create_new_version=True)\n",
    "    return train_data, test_data\n",
    "    \n",
    "train_data, test_data = split_dataset(feedback_sql_ds)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from azureml.train.automl import AutoMLConfig\n",
    "\n",
    "automl_config = AutoMLConfig(task = 'regression',\n",
    "                             debug_log = 'automl_errors.log',\n",
    "                             training_data = train_data,\n",
    "                             label_column_name = label,\n",
    "                             verbosity = logging.INFO,\n",
    "                             enable_early_stopping = True, \n",
    "                             experiment_timeout_minutes = 60,\n",
    "                             max_concurrent_iterations = 4,\n",
    "                             max_cores_per_iteration = -1,\n",
    "                             n_cross_validations = 5,\n",
    "                             primary_metric ='normalized_root_mean_squared_error',\n",
    "                             preprocess=True\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.experiment import Experiment\n",
    "experiment = Experiment(ws, \"ai-impact-score-experiment-dc-sql\")\n",
    "\n",
    "local_run = experiment.submit(automl_config, show_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_run, fitted_model = local_run.get_output()\n",
    "\n",
    "model = best_run.register_model(model_name='best_sql_dc_impact_score_model', model_path='./outputs/model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Registered model:\\n --> Name: {}\\n --> Version: {}\\n --> URL: {}\".format(model.name, model.version, model.url))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:MLHyperparameterTuning] *",
   "language": "python",
   "name": "conda-env-MLHyperparameterTuning-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}